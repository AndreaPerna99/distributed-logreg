# Distributed Classification via Logistic Regression with Gradient Tracking

[![Python](https://img.shields.io/badge/Python-3.10-blue?logo=python)](https://www.python.org/)  
Decentralized machine learning algorithm for logistic regression using Gradient Tracking across a network of agents.

---

## ğŸ§  About the Project

This project implements a **distributed classification** system using **Gradient Tracking (GT)** in a multi-agent framework. It is divided into three main parts:

- ğŸ”„ **Distributed Optimization** â€” Solving a quadratic consensus problem using GT.
- ğŸ§® **Centralized Classification** â€” Logistic regression via gradient descent on synthetic data.
- ğŸ›° **Distributed Classification** â€” Classifying data distributed across agents using GT.

Implemented in a fully interactive **Jupyter Notebook**, this project demonstrates the real-world applicability of decentralized learning algorithms for nonlinear classification tasks.

---

## ğŸ—‚ Project Structure

```
ğŸ“¦ distributed-logreg/
â”œâ”€â”€ ğŸ“ Task1/                                      # Main notebook and materials
â”‚   â”œâ”€â”€ ğŸ““ DAS_Task1_Group3.ipynb                  # Jupyter Notebook
â”‚   â”œâ”€â”€ ğŸ“„ Task1.pdf                               # Notebook execution PDF
â”‚   â”œâ”€â”€ ğŸ“˜ report_group_03.pdf                     # Full project report (pages 7â€“18 relevant)
â”‚   â””â”€â”€ ğŸ“ images/                                 # Optional folder for future plots
â”œâ”€â”€ ğŸ“„ README.md                                   # You are here!
```                                                 
## âš™ï¸ How to Run

1. Move into the task folder:

```
$ cd Task1/
$ jupyter notebook
```

2. Open the notebook `DAS_Task1_Group3.ipynb` and run each section sequentially.

## ğŸ§ª Tasks Breakdown

### ğŸš€ Task 1.1 â€“ Distributed Optimization

- Implements Gradient Tracking on a network of agents solving a decentralized quadratic optimization problem.
- Agents reach consensus using a doubly-stochastic weighted graph.
- Test different topologies: cycle, path, star.
- Plots include agents' states, gradient norms, and cost convergence.

### ğŸ§  Task 1.2 â€“ Centralized Classification

- Generate 2D datasets labeled by nonlinear conic functions (e.g., ellipse, parabola).
- Apply centralized Gradient Descent for logistic regression.
- Evaluate classification on train/test sets and compute key metrics.

### ğŸŒ Task 1.3 â€“ Distributed Classification

- Extend Task 1.1 and 1.2 to a fully distributed setting.
- Split data among agents with or without label noise.
- Use GT to minimize logistic loss collaboratively.
- Visualize decision boundaries and metric evolution.
- Classify with and without label noise and compare results.

## ğŸ“Š Output & Evaluation

- ğŸ“‰ Plots: Cost evolution, gradient norms, decision boundaries.
- ğŸ“‹ Metrics: Classification error, accuracy, precision, recall, F1-score (train & test).
- âœ… Distributed version matches centralized accuracy with small performance drop under noise.

## ğŸ“ Resources

- ğŸ“˜ Full Report (PDF): ./Task1/report_group_03.pdf (pages 7â€“18)
- ğŸ“„ Notebook Output (PDF): ./Task1/Task1.pdf

## ğŸ‘¨â€ğŸ“ Authors

Group 3 â€“ MSc Automation Engineering, University of Bologna

- Andrea Perna
- Gianluca Di Mauro
- Meisam Tavakoli

ğŸ“§ andrea.perna3@studio.unibo.it

## ğŸ‘©â€ğŸ« Supervisors

- Prof. Giuseppe Notarstefano
- Prof. Ivano Notarnicola
- Dr. Lorenzo Pichierri

## ğŸ“œ License

All rights reserved. Educational use only.
